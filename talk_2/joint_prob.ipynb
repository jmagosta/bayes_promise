{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create node Conditional Probability Tables (CPTs) from the classification tree splits\n",
    "\n",
    "The classification tree generates counts for each conjunction of splits that determine the full joint over the model's uncertain variables. \n",
    "\n",
    "JMA 24 March 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from the python standard library\n",
    "import math, re, os, sys \n",
    "from pathlib import Path\n",
    "import itertools            # to flatten lists\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# Import array and dataframe packages\n",
    "import numpy as np\n",
    "# import numpy.linalg as la\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ( Parse classification tree output to obtain counts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CART's leaf node counts to estimate the variables' joint probability distribution\n",
    "\n",
    "The count of [peach, lemon] in the four leaf nodes represent the probability of discrete variables based on the splitting thresholds that CART generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Joint  p; ',\n",
       " tensor([[[0.3040, 0.0060],\n",
       "          [0.0020, 0.1030]],\n",
       " \n",
       "         [[0.0070, 0.4310],\n",
       "          [0.1420, 0.0050]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dims: (ignition, carb, y).  \n",
    "# y == state variable\n",
    "joint_cnts = torch.tensor([[[304, 6], [2, 103]], [ [7, 431],[142,5]]])\n",
    "# Normalize the counts by the number of training samples total\n",
    "joint_p = joint_cnts / joint_cnts.sum()\n",
    "'Joint  p; ', joint_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Raw prior, y: ', tensor([0.4550, 0.5450]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The implicit prior -- the state empirical distribution. \n",
    "Py = joint_p.sum(axis=(0,1))\n",
    "'Raw prior, y: ', Py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a \"likelihood message\" to the prior\n",
    "\n",
    "The state variable (lemon, peach) prior can be adjusted when learning the classification tree to meet the actual belief, regardless of the class imbalance in the training set. \n",
    "\n",
    "Computationally this is the Bayes network analog of sending a likelihood message to the prior distribution of the joint probability.  Equivalently this can be done by setting the \"params\" argument in `rpart()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('New prior: ', tensor([0.2000, 0.8000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjust priors \n",
    "adjustment0 = 0.2 / Py[0]\n",
    "adjustment1 = (1- Py[0]*adjustment0)/Py[1]\n",
    "# Pc_adjusted\n",
    "adjustment = torch.tensor([adjustment0.item(), adjustment1.item()])\n",
    "'New prior: ',adjustment * Py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.8000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the adjustment to the joint\n",
    "new_joint_p = joint_p * adjustment.expand(2,2,2)\n",
    "# Check the new prior\n",
    "new_joint_p.sum(axis=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning the joint on the features obtains the posterior consistent with the prior:\n",
    "\n",
    "$ P (y\\ |\\ ig, carb )$\n",
    "\n",
    "The \"features\" are the independent variables, \"ignition_test\" and \"carburator_test\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9382, 0.0618],\n",
       "         [0.0058, 0.9942]],\n",
       "\n",
       "        [[0.0048, 0.9952],\n",
       "         [0.8948, 0.1052]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition to get p(y | g, c)\n",
    "# Note, this is just the conditional probabilities at the node leaves. \n",
    "y_norm = new_joint_p.sum(2)\n",
    "Py_given_gc = new_joint_p / y_norm.expand(2,2,2).permute(1,2,0)\n",
    "Py_given_gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete factoring the joint\n",
    "\n",
    "Condition to obtain the CPT for \"ignition_test\":\n",
    "\n",
    "$ P ( ig\\ |\\ carb)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4836, 0.5164],\n",
       "        [0.9011, 0.0989]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition to get (g | c)\n",
    "# Sum out y: remaining dimensions are g, c\n",
    "Pgc = new_joint_p.sum(2)\n",
    "gc_norm = Pgc.sum(1)\n",
    "Pg_given_c = Pgc / gc_norm.expand(2,2).permute(1,0)\n",
    "Pg_given_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full factorization of the three model variables is\n",
    "\n",
    "$ P( y, ig, carb) = P (y\\ |\\ ig, carb ) P ( ig\\ |\\ carb) P( carb )$\n",
    "\n",
    "The factors are the CPTs for the nodes in the influence diagram. \n",
    "\n",
    "Here is the final term in the factorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7782, 0.2218])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the preposterior on c\n",
    "Pc = new_joint_p.sum(axis=(0,2))\n",
    "Pc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
