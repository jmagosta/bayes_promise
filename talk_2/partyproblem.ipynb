{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Single Decisions\n",
    "\n",
    "## The \"Party Problem\" example\n",
    "\n",
    "JMA 11 Jan 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from the python standard library\n",
    "import math, re, os, sys \n",
    "from pathlib import Path\n",
    "import itertools            # to flatten lists\n",
    "\n",
    "# Import array and dataframe packages\n",
    "import numpy as np\n",
    "# import numpy.linalg as la\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Use to parse xdsl files\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn \n",
    "# Import the bokeh python wrappers for javascript plots\n",
    "#  - a preferred visualization tool\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, VBar, Span\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "NETWORK_FILE = 'PartyProblem_asym.xdsl' # 'PartyProblem_asym.xdsl'  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Walk the xdsl elements of a network\n",
    "def extract_net(xdsl_file):\n",
    "    '''Finds the first element under the top level that contains a list of nodes,\n",
    "    and returns a dict of node element objects.'''\n",
    "    tree = et.parse(xdsl_file)\n",
    "    root = tree.getroot()\n",
    "    # BN structure is contained under the node element\n",
    "    node_tree = root.findall('nodes')[0]\n",
    "    print(f'found {node_tree.tag}')\n",
    "    extensions = root.find('extensions')\n",
    "    extensions_tree = extensions.find('genie')\n",
    "    node_extensions = extensions_tree.findall('node')\n",
    "    return  list(node_tree), list(node_extensions)\n",
    "    \n",
    "# BN structure is contained under the node branch\n",
    "parsed = extract_net(NETWORK_FILE)\n",
    "nodes, extensions = parsed\n",
    "# tags tell the node type. \n",
    "[( k.get('id'), k.tag) for k in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the XML of an element\n",
    "# CPT contents are stored in row major order (first row, second row, ...)\n",
    "# Parents are the first matrix dimension -- matrix is Row Markov\n",
    "# et.dump(\n",
    "parsed\n",
    "     # nodes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes networks object\n",
    "\n",
    "It contains \n",
    "\n",
    "- the parse of the network as a dictionary with node names as keys\n",
    "- The graph object showing network structure\n",
    "- Potential objects for computation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPT parent order\n",
    "# Dimensions are stored in matrix dimension order. \n",
    "[et.dump(n) for n in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include state and variable names to tensor dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labelled dimension object.\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Potential:\n",
    "\n",
    "    def __init__(self, cpt, shape):\n",
    "        ' cpt  - multidim tensor, shape: OrderedDict '\n",
    "        self.p = cpt\n",
    "        self.shape = shape\n",
    "        self.dim_names = shape.keys()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.shape) + '\\n\\t' + repr(self.p)\n",
    "    \n",
    "def new_Potential(prob_list, dim_list, dim_names ):\n",
    "    'factory for creating potential from parsed xml components'\n",
    "    p = torch.tensor(prob_list).reshape(dim_list)\n",
    "    sh = OrderedDict(zip(dim_names, dim_list))\n",
    "    return Potential(p, sh)\n",
    "    \n",
    "def get_potential(a_node, n_dict):\n",
    "    'Find the probability np array in the node, and label it using parents in the graph'\n",
    "    # The states of the RV label the columns, so that the matrix is row-markov\n",
    "    the_cpt = n_dict[a_node]['potential']\n",
    "    return the_cpt\n",
    "\n",
    "# Place margin probabilities in the last dimension\n",
    "md = new_Potential([0.1, 0.9, 0.4, 0.6], [2,2], ['condition', 'margin'])\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing routines\n",
    "# NOTE: all extract_* functions have side-effects that modify node_dict\n",
    "class BN (object):\n",
    "\n",
    "    def __init__(self, name_dict={}):\n",
    "        self.n_dict = name_dict\n",
    "        self.network = None\n",
    "        # Build a reverse topological order to the DAG\n",
    "        self.node_order = None\n",
    "        self.edges = None\n",
    "\n",
    "    def set_kind(self, a_node):\n",
    "        'Both create the node key and its kind.' \n",
    "        self.n_dict[a_node.get('id')] = {'kind': a_node.tag}\n",
    "\n",
    "    def extract_parents(self, a_node):\n",
    "        parent_list = []\n",
    "        p = a_node.find('parents')\n",
    "        if p is not None:\n",
    "            parent_list = p.text.split(' ') \n",
    "        self.n_dict[a_node.get('id')]['parents' ] = parent_list\n",
    "        return self.n_dict\n",
    "\n",
    "    def extract_states(self, a_node):\n",
    "        state_list = []\n",
    "        for element in a_node:\n",
    "            if element.tag == 'state':\n",
    "                state_list.append(element.get('id'))\n",
    "        self.n_dict[a_node.get('id')]['states' ] = state_list\n",
    "        return self.n_dict\n",
    "\n",
    "    def state_size(self, node_name):\n",
    "        # Deterministic nodes such as utilities have only one state. \n",
    "        \n",
    "        # if a_node.tag == 'utilities':\n",
    "        #     return 1\n",
    "        # else:\n",
    "        #     node_name = a_node.get('id') \n",
    "        return len(self.n_dict[node_name]['states'])\n",
    "\n",
    "    def get_parents(self, a_node):\n",
    "        return self.n_dict[a_node]['parents' ]\n",
    "    \n",
    "    def build_tensor(self, a_node, elements):\n",
    "        node_name = a_node.get('id')\n",
    "        dim_names = [node_name]\n",
    "        # Need the parents to dimension the cpt\n",
    "        state_counts = [self.state_size(node_name)]    \n",
    "        parents = self.get_parents(node_name)\n",
    "        dim_names.extend(parents)  \n",
    "        for p in parents:\n",
    "            state_counts.append(self.state_size(p))      #list of dimensions\n",
    "        # print('S', state_counts)\n",
    "        try:\n",
    "            # if len(prob_list) == state_counts[0]:             # One dimension, no conditioning \n",
    "            potential = new_Potential(elements, state_counts, dim_names)   \n",
    "            #     cpt = torch.tensor(prob_list).reshape(state_counts)\n",
    "            self.n_dict[node_name]['potential' ] = potential\n",
    "        except Exception as e:\n",
    "            print('Err ', e)\n",
    "            print(f'list of len {elements} is not a consistent with {state_counts}.')\n",
    "\n",
    "    def extract_probabilities(self, a_node):\n",
    "        # Probabilities are stored as a flat list, in row major order, e.g. \n",
    "        # for each conditioning, the probs for each state are listed together\n",
    "        # sequentially. \n",
    "        p = a_node.find('probabilities')\n",
    "        if p is not None:\n",
    "            prob_list = [float(k) for k in p.text.split(' ')]\n",
    "            self.build_tensor(a_node, prob_list)\n",
    "        # except Exception as e:\n",
    "        #     print('Err ', e)\n",
    "        #     print(f'list of len {prob_list} is not a consistent with {state_counts}.')\n",
    "        return self.n_dict\n",
    "\n",
    "    def extract_utilities(self, a_node):\n",
    "        u = a_node.find('utilities')\n",
    "        self.n_dict[a_node.get('id')]['states' ] = ['utility']   # a dimension with just one state. \n",
    "        if u is not None:\n",
    "            u_list = [float(k) for k in u.text.split(' ')]\n",
    "            self.build_tensor(a_node, u_list)\n",
    "            # TODO The utilities list dimension with  parent states. \n",
    "        # self.n_dict[a_node.get('id')]['utilities' ] = u_list\n",
    "        return self.n_dict\n",
    "\n",
    "    # Note: Node extensions also have the display name of the node, which is an\n",
    "    # alternative to it's id. \n",
    "    def extract_positions(self, a_node_extension):\n",
    "        u = a_node_extension.find('position')\n",
    "        if u is not None:\n",
    "            u_list = [int(k) for k in u.text.split(' ')]\n",
    "            # The utilities list cannot be dimensioned until we know it's parent states. \n",
    "        self.n_dict[a_node_extension.get('id')]['position' ] = u_list\n",
    "        return self.n_dict\n",
    "\n",
    "    # Create an a-cyclic graph from the parents of each node. \n",
    "\n",
    "    def weave(self):\n",
    "        'From the reaped list of nodes connect them into a network.'\n",
    "        # Assemble edge lists\n",
    "        edges = []\n",
    "        for (k, attr) in self.n_dict.items():\n",
    "            parents = self.get_parents(k)\n",
    "            if len(parents) > 0:\n",
    "                [edges.append((z, k )) for z in parents] # Arc direction: z -> k\n",
    "        print('Edges: ',edges, '\\n')\n",
    "        self.edges = edges\n",
    "        return edges\n",
    "    \n",
    "    # def get_tensor(self, variable):\n",
    "    #     return self.n_dict[variable]['potential']\n",
    "    \n",
    "# create a BN object from the parsing \n",
    "def reap(the_parse):\n",
    "    'Factory to parse the attributes of each node, returning a list with the attributes in a dict.'\n",
    "    bn = BN()\n",
    "    the_nodes, the_extensions = the_parse\n",
    "    for a_node in the_nodes:\n",
    "        # Set the node kind\n",
    "        bn.set_kind(a_node)\n",
    "        # node_dict[a_node.get('id')] = {'kind': a_node.tag}\n",
    "        bn.extract_parents(a_node)\n",
    "        # CPT and decision nodes have states\n",
    "        if (a_node.tag == 'cpt') or (a_node.tag == 'decision'):\n",
    "            bn.extract_states(a_node)\n",
    "        if (a_node.tag == 'cpt'):\n",
    "            bn.extract_probabilities(a_node)\n",
    "        if (a_node.tag == 'utility'):\n",
    "            bn.extract_utilities(a_node)\n",
    "    for an_ex in the_extensions:\n",
    "        bn.extract_positions(an_ex)\n",
    "    bn.edges = bn.weave()\n",
    "    return bn\n",
    "\n",
    "bn = reap(parsed)\n",
    "bn.n_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotable graph\n",
    "G = nx.DiGraph(bn.edges)\n",
    "\n",
    "for n in G:\n",
    "    print(n, ': ',nx.ancestors(G,n), '\\t', nx.descendants(G,n), '\\n')\n",
    "#     nx.nodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The networkX object does not contain the node attributes extracted from the XDSL\n",
    "DG = nx.DiGraph(bn.edges)\n",
    "DG.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_centers(node_dict):\n",
    "    xmax = ymax = - pow(2,16)\n",
    "    xmin = ymin =   pow(2,16)\n",
    "    center_dict = {}\n",
    "    for k,attr in node_dict.items():\n",
    "        v = attr['position']\n",
    "        x = (v[0] + v[2])/2\n",
    "        y = -(v[1] + v[3])/2\n",
    "        center_dict[k]  = np.array((x,y))\n",
    "        xmin = min(x, xmin)\n",
    "        ymin = min(y, ymin)\n",
    "        xmax = max(x, xmax)\n",
    "        ymax = max(y, ymax)\n",
    "    return center_dict\n",
    "\n",
    "positions = node_centers(bn.n_dict) \n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "nx.draw_networkx_labels(DG, pos=positions)\n",
    "nx.draw_networkx_nodes(DG, pos=positions, node_color='lightgrey')\n",
    "nx.draw_networkx_edges(DG, pos=positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract matrices as tensors.  _List all tensors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See p. 270, Howard & Abbas\n",
    "# P( detector | weather)\n",
    "\n",
    "def pr_node_tensor(the_n):\n",
    "    print(a_node, '\\n\\t', get_potential(the_n, bn.n_dict),'\\n')\n",
    "\n",
    "\n",
    "def pr_named_tensors(name_dict = bn.n_dict):\n",
    "    'Show all the model tensors'\n",
    "    for a_node in name_dict:\n",
    "        if name_dict[a_node]['kind'] == 'cpt' or name_dict[a_node]['kind'] == 'utility':\n",
    "            print(a_node, '\\n\\t', get_potential(a_node, bn.n_dict),'\\n')\n",
    "\n",
    "pr_named_tensors(bn.n_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format one-dim tensors \n",
    "# from collections import deque\n",
    "def one_dim_table(the_potential, the_var, n_dict=bn.n_dict, **args):\n",
    "    def que_copy(prefix, queue):\n",
    "       if not isinstance(queue, list):\n",
    "           queue = [queue]\n",
    "       queue.insert(0, prefix)\n",
    "       return queue\n",
    "    states = n_dict[the_var]['states']\n",
    "    values = the_potential.tolist()\n",
    "    # Flatten nested lists\n",
    "    while len(values)  == 1:   # TODO is this test necessary?\n",
    "        values = values[0]\n",
    "    print(f' *** {the_var} ***')\n",
    "    values = [que_copy(s, v) for s, v in zip(states, values)]\n",
    "    print(tabulate(values, **args))\n",
    "\n",
    "one_dim_table(get_potential('Weather', bn.n_dict).p, 'Weather', tablefmt= '.4f', headers= ['State', 'Value'])\n",
    "# get_potential('Weather', bn.n_dict).p.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For joining by aligning potentials as named tensors\n",
    "\n",
    "def dim_index(potential_cpt, candidate):\n",
    "    # Starting with 0 as the first location, so the last location equals the length of the shape\n",
    "    cpt_dims = potential_cpt.dim_names\n",
    "    # if its included is it not already last?\n",
    "    if candidate in cpt_dims and candidate != list(cpt_dims)[-1]:\n",
    "        return list(cpt_dims).index(candidate) \n",
    "    else:\n",
    "        # Either candidate not found or its already at the end, so do nothing.\n",
    "        return None\n",
    "\n",
    "def move_named_dim_to_end(the_named_tensor, the_dimension):\n",
    "    'Transpose the potential place the dimension last'\n",
    "    the_dim_index = dim_index(the_named_tensor, the_dimension)\n",
    "    if the_dim_index is not None:\n",
    "        # Create a modified shape OrderedDict\n",
    "        shape = the_named_tensor.shape.copy()\n",
    "        shape.move_to_end(the_dimension)\n",
    "        # Rotate the tensor dimensions\n",
    "        p_transpose = list(range(len(shape)))               # The unperturbed list\n",
    "        p_transpose.append(p_transpose.pop(the_dim_index))  # Move index to end\n",
    "        # Transpose CPT\n",
    "        x = the_named_tensor.p.permute(p_transpose)\n",
    "        return Potential(x, shape)\n",
    "    else:\n",
    "        # A no op\n",
    "        return the_named_tensor \n",
    "    \n",
    "def marginalize_last(p1, p2):\n",
    "    '''For a potential matching the last dimension of the other, join them,\n",
    "    then marginalized out the last dimension'''\n",
    "    if list(p1.shape)[-1] != list(p2.shape)[-1]:           # Compare shapes by indexed value\n",
    "        print(f'Err, last shapes do not match:\\t{list(p1.shape)[-1]} != {list(p2.shape)[-1]}')\n",
    "        return None\n",
    "    else:\n",
    "        new_tensor = (p1.p * p2.p).sum(-1)\n",
    "        # The symmetric set difference - those not common to both. \n",
    "        s1 = set(p1.shape.items())\n",
    "        s2 = set(p2.shape.items())\n",
    "        new_shape = OrderedDict(s1.union(s2) - s1.intersection(s2))\n",
    "    return Potential(new_tensor, new_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility matrix, as a Potential\n",
    "outcome_potential = get_potential('Preferences', bn.n_dict)\n",
    "outcome_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No problem with mapping single arg functions over tensors!  \n",
    "def delta_utility(x, exponand = 0.5, normalize = 50):\n",
    "    dims = x.shape\n",
    "    u = 4/3*(1 - pow(exponand, (x.p/normalize)))\n",
    "    return Potential(u, dims)\n",
    "\n",
    "u = delta_utility(outcome_potential)\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the party problem\n",
    "\n",
    "To determine the optimal policy\n",
    "\n",
    "* join Detector and Weather CPTs \n",
    "* join DW with Utility (Decision is implicit in Utility, w/ unit values for all options)\n",
    "* marginalize out unobserved Weather (or do this after decn, to get VOI)\n",
    "* Maximize over options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First solution - only prior, no observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that marginalize_last is symmetric in its arguments. \n",
    "\n",
    "preference_transpose = move_named_dim_to_end(get_potential(\"Preferences\", bn.n_dict), \"Weather\")\n",
    "preference_transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_utility = marginalize_last(preference_transpose, get_potential(\"Weather\",bn.n_dict))\n",
    "one_dim_table(prior_utility.p, 'Party_location') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_posterior, shape =  weather_cpt.p @ detector_cpt.p.permute(2,0,1) @ adjustor_cpt.p\n",
    "# pre_posterior\n",
    "\n",
    "def marginalize(child_potential, parent_potential):\n",
    "    cpt = (child_potential.p * parent_potential.p).sum(-1)\n",
    "    # TODO remove parent shape from child\n",
    "    sh = OrderedDict(set(child_potential.shape.items()) - set(parent_potential.shape.items()))\n",
    "    return Potential(cpt, sh)\n",
    "\n",
    "def shift_to_end(the_shape, the_var):\n",
    "    the_shape.move_to_end(the_var)\n",
    "    return the_shape\n",
    "\n",
    "\n",
    "def join_parent(the_conditional, the_parent, name_dict= bn.n_dict):\n",
    "    'Assume the parent rv is the last dim in the conditional, and marginalize out that dim'\n",
    "    # Find the parent and transpose it to last dim\n",
    "    c_potential = get_potential(the_conditional, name_dict)\n",
    "    p_potential = get_potential(the_parent, name_dict)\n",
    "    found_dim = dim_index(c_potential,the_parent)\n",
    "    # Is found dim not already in the last dim? \n",
    "    new_shape = c_potential.shape\n",
    "    if found_dim is not None:   # TODO does this work if the found dim is first?\n",
    "        # Move found_dim to last dimension\n",
    "        new_shape = shift_to_end(new_shape, the_parent)\n",
    "        c_transpose = list(range(len(new_shape)))\n",
    "        c_transpose.append(c_transpose.pop(found_dim))\n",
    "        # Transpose CPT\n",
    "        c_potential.p.permute(c_transpose)\n",
    "        # TODO - create a new potential? \n",
    "    new_joint =  Potential(c_potential.p * p_potential.p, new_shape)\n",
    "    return new_joint\n",
    "\n",
    "    # return the joint potential, with the conditioning prob last. \n",
    "# join_parent(marginalize_parent(detector_cpt.permute(2,1,0), adjustor_cpt), weather_cpt)\n",
    "dw_joint = join_parent(\"Detector\", 'Weather')\n",
    "dw_joint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P( Weather | Detector) - column markov\n",
    "# See p 270 Figure 13.6\n",
    "#   Transpose the joint to get the Detector in last dim, and normalize \n",
    "pre_posterior_p = dw_joint.p.sum(-1) \n",
    "p = dw_joint.p.transpose(0,1) * (1/pre_posterior_p)\n",
    "# reverse the shape \n",
    "posterior = Potential(p, OrderedDict(list(reversed(dw_joint.shape.items()))) )\n",
    "print(pre_posterior_p, '\\n\\n', posterior)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One approach is to add a unsqueeze dim to match Detector at the end of preference transpose\n",
    "# BINGO\n",
    "extended_preference = preference_transpose.p.unsqueeze(-1).unsqueeze(-1)\n",
    "print(extended_preference.shape)\n",
    "# Sum out the weather dimension\n",
    "policy_values = (extended_preference * posterior.p).sum(2)\n",
    "print('E[ V | Party_location, Detector] = ')\n",
    "policy_values\n",
    "# Next we need to weight the optimal in each column by the pre-posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Need to format list entries before passing to tabulate. \n",
    "# TODO looks like the State labels are flipped. \n",
    "detector_states= bn.n_dict['Detector']['states'].copy()\n",
    "detector_states.insert(0, 'State')\n",
    "one_dim_table(policy_values.squeeze(0), \n",
    "    'Party_location', \n",
    "    floatfmt= \".3f\", \n",
    "    headers= detector_states)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_values.squeeze(0).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the last dim?  Need to remove it. \n",
    "fig, ax = plt.subplots(1,2, figsize = (6, 2.6))\n",
    "policy_values_2d_a = pd.DataFrame(policy_values.squeeze(0)[:,:,1], columns = bn.n_dict['Detector']['states'], \n",
    "                                index = bn.n_dict['Party_location']['states'])\n",
    "sn.heatmap(policy_values_2d_a, annot=True, xticklabels=True, yticklabels=True, ax=ax[0])\n",
    "policy_values_2d_b = pd.DataFrame(policy_values.squeeze(0)[:,:,0], columns = bn.n_dict['Detector']['states'], \n",
    "                                index = bn.n_dict['Party_location']['states'])\n",
    "sn.heatmap(policy_values_2d_b, annot=True, xticklabels=True, yticklabels=True, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max value in each column. \n",
    "decn = policy_values.max(1)\n",
    "decn.values, decn.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value with information. \n",
    "# However utility should be applied after computing expected values to get certain equivalents\n",
    "# sigh\n",
    "# 0.7782 * 0.44 + 0.6557 * 0.56\n",
    "decn.values @ get_potential('Weather', bn.n_dict).p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
